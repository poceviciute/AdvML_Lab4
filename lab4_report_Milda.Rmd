---
title: "GP_lab_Milda"
author: "Milda Poceviciute"
date: "16 October 2018"
output: pdf_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(kernlab)
library(AtmRay)
set.seed(12345)
```

## Question 1

### Part 1

The code below is the implementation of the Gaussian process regression model:

$y = f(x) + \epsilon$ with $\epsilon \sim N(0,\sigma_n^2)$, $f \sim GP(0,k(x,x^{'}))$

```{r,echo=TRUE}
# Setting up the kernel
SquaredExpKernel <- function(x1,x2,sigmaF=1,l=3){
    n1 <- length(x1)
    n2 <- length(x2)
    K <- matrix(NA,n1,n2)
    for (i in 1:n2){
        K[,i] <- sigmaF^2*exp(-0.5*( (x1-x2[i])/l)^2 )
    }
    return(K)
}


posteriorGP <- function(X, y, XStar, hyperParam, sigmaNoise) {
    k_star <- SquaredExpKernel(X, XStar, sigmaF = hyperParam$sigmaF, l = hyperParam$l)
    K <- SquaredExpKernel(X, X, sigmaF = hyperParam$sigmaF, l = hyperParam$l)
    noise <- diag(nrow(K)) * sigmaNoise
    L <- t(chol(K + noise))
    
    # Predictive mean
    alpha <- solve(L) %*% y
    posterior_mean <- t(k_star) %*% alpha
    # Predictive variance
    v <- solve(L) %*% k_star
    kk <- SquaredExpKernel(XStar, XStar, sigmaF = hyperParam$sigmaF, l = hyperParam$l)
    posterior_var <- kk - t(v) %*% v
    
    return(list(posterior_mean = posterior_mean, posterior_var = posterior_var))
    
}
```

### Part 2

In this part we assume that prior hyper-parameters are $\sigma_f = 1$, $\ell = 0.3$, and we update the posterior with observation $(x,y)=(0.4,0.719)$. Here $\sigma_n$ is assumed to be 0.1. The plot below shows the observation, the posterior mean (the black line), and the 95% probability bands (point-wise) plotted over a grid $[-1,1]$. The probability bands are quite wide for most of the x grid points, except from the location of the observed point $(x,y)$. This observation decreases the uncertainty and bands become narrower.

```{r}

# Information for updating the prior

X_observations <- c(0.4)
y_observations <- c(0.719)
hyperParam2 <- list(sigmaF = 1, l = 0.3)

# Use the data set and new given point to compute the posterior over the grid [-1,1]
xGrid <- seq(-1,1,length=50)
posteriorP2 <- posteriorGP(X = X_observations, y = y_observations, XStar = xGrid, hyperParam = hyperParam2, sigmaNoise = 0.1)

# find probability bounds
error <- qnorm(0.975)*sqrt(diag(posteriorP2$posterior_var))
upp_band <- as.vector(posteriorP2$posterior_mean) + as.vector(error) 
low_band <- as.vector(posteriorP2$posterior_mean) - as.vector(error)

# Plot the posterior means over a grid
plot(x=xGrid,y=posteriorP2$posterior_mean, type="l",ylim=c(-2,4),xlab="x", ylab="Posterior Mean")
# Add 95% probability bounds (check bayesian course, we must have doen it there)
lines(x=xGrid,y=upp_band, type="l",col="blue")
lines(x=xGrid,y=low_band, type="l",col="blue")
abline(v=0.4,col="red")
legend("topright",c("Observations","Prob. bands","Post. Mean"),
        col = c("red","blue","black"),lty = c(1, 1, 1), lwd=c(2,2,2))


```

### Part 3

The posterior is updated with one more observation $(x,y)=(-0.6,-0.044)$. The plot below shows again the observations, the posterior mean(the black line), and the 95% probability bands (point-wise) over the grid $[-1,1]$. This time the uncertainty is smaller for the two observed $x$ values: the probability bands are narrower there.

```{r}
# Update the information

X_observations[2] <- -0.6
y_observations[2] <- -0.044

posteriorP3 <- posteriorGP(X = X_observations, y = y_observations, XStar = xGrid, hyperParam = hyperParam2, sigmaNoise = 0.1)
# find probability bounds
error3 <- qnorm(0.975)*sqrt(diag(posteriorP3$posterior_var))
upp_band3 <- as.vector(posteriorP3$posterior_mean) + as.vector(error3) 
low_band3 <- as.vector(posteriorP3$posterior_mean) - as.vector(error3) 

# Plot the posterior means over a grid
plot(x=xGrid,y=posteriorP3$posterior_mean, type="l",ylim=c(-2,4),xlab="x", ylab="Posterior Mean")
# Add 95% probability bounds (check bayesian course, we must have doen it there)
lines(x=xGrid,y=upp_band3, type="l",col="blue")
lines(x=xGrid,y=low_band3, type="l",col="blue")
abline(v=0.4,col="red")
abline(v=-0.6,col="red")
legend("topright",c("Observations","Prob. bands","Post. Mean"),
        col = c("red","blue","black"),lty = c(1, 1, 1), lwd=c(2,2,2))

```

### Part 4

The plot shows the posterior mean and the 95% probability bands after having 5 observations. The uncertainty has decreased a lot in comparison to parts 2 and 3.

```{r}
X_observations[1] <- -1
X_observations[3:5] <- c(-0.2,0.4,0.8)
y_observations[3:5] <- c(-0.940, 0.719, -0.664)

posteriorP4 <- posteriorGP(X = X_observations, y = y_observations, XStar = xGrid, hyperParam = hyperParam2, sigmaNoise = 0.1)
# find probability bounds
error4 <- qnorm(0.975)*sqrt(diag(posteriorP4$posterior_var))
upp_band4 <- as.vector(posteriorP4$posterior_mean) + as.vector(error4) 
low_band4 <- as.vector(posteriorP4$posterior_mean) - as.vector(error4) 

# Plot the posterior means over a grid
plot(x=xGrid,y=posteriorP4$posterior_mean, type="l",ylim=c(-2,3),xlab="x", ylab="Posterior Mean")
# Add 95% probability bounds (check bayesian course, we must have doen it there)
lines(x=xGrid,y=upp_band4, type="l",col="blue")
lines(x=xGrid,y=low_band4, type="l",col="blue")
legend("topright",c("Prob. bands","Post. Mean"),
        col = c("blue","black"),lty = c(1, 1), lwd=c(2,2))

```

### Part 5

The plot below shows the posterior mean and the 95% probability bands after having the 5 observations as in part 4, but changing the hyper-parameters to $\sigma_f = 1$, $\ell = 1$. The higher the $\ell$ value, the smoother the posterior mean becomes. In the kernel the distance between the points is divided by $\ell$. This means that the higher the value of this hyper-parameter is, the  kernel values converge to the $\sigma_f$: the distance between the observations becomes less and  less important. Hence, the posterior mean becomes more smooth.

```{r}
hyperParam5 <- list(sigmaF = 1, l = 1)

posteriorP5 <- posteriorGP(X = X_observations, y = y_observations, XStar = xGrid, hyperParam = hyperParam5, sigmaNoise = 0.1)
# find probability bounds
error5 <- qnorm(0.975)*sqrt(diag(posteriorP5$posterior_var))
upp_band5 <- as.vector(posteriorP5$posterior_mean) + as.vector(error5) 
low_band5 <- as.vector(posteriorP5$posterior_mean) - as.vector(error5) 

# Plot the posterior means over a grid
plot(x=xGrid,y=posteriorP5$posterior_mean, type="l", ylim = c(-2,1),xlab="x", ylab="Posterior Mean")
# Add 95% probability bounds (check bayesian course, we must have doen it there)
lines(x=xGrid,y=upp_band5, type="l",col="blue")
lines(x=xGrid,y=low_band5, type="l",col="blue")
legend("topright",c("Prob. bands","Post. Mean"),
        col = c("blue","black"),lty = c(1, 1), lwd=c(2,2))

```

## Question 2

### Part 1


```{r}
dataQ2 <- read.csv("TempTullinge.csv", header=TRUE, sep=";")
time <- 1:2190
day <- 1:365
Stime <- seq(1,2190,by=5)
Sday <- seq(1,365,by=5)
x1 <- c(1,3,4)
x2 <- c(2,3,4)
```

In this question, the exponential kernel function is defined: 
```{r,echo=TRUE}


# Function that returns kernel class object
Matern32 <- function(sigmaf = 1, ell = 1) 
{
  rval <- function(x1,x2){
    n1 <- length(x1)
    n2 <- length(x2)
    K <- matrix(NA,n1,n2)
    for (i in 1:n2){
      K[,i] <- sigmaf^2*exp(-0.5*( (x1-x2[i])/ell)^2 )
    }
    return(K)
  }
  class(rval) <- "kernel"
  return(rval)
} 

MaternFunc = Matern32(sigmaf = 1, ell = 2)
```

It is used to compute the covariance, firstly between (1,2), and afterwards between points $x=(1,3,4)$ and $x^*=(2,3,4)$. The results are printed:
```{r}

#evaluate it in the point x = 1; x' = 2
cat("The kernel between points (1,2) is :", MaternFunc(1,2))
cat("\n")
cat("\n")
# use the kernelMatrix function to compute the covariance matrix K(X;X*)
covM <- kernelMatrix(MaternFunc, x1,x2)
cat("The kernel between points (1,3,4) and (2,3,4) is:") 
cat("\n")
covM
```

When we used to fit GP with different $\ell$ in the kernel, we get that smaller $\ell$ gives less smooth f: we are overfitting the data. And larger  $\ell$ (length scale) gives more smoothness.

### Part 2

The following model is used for obtaining the posterior mean:

$temp = f(time) + \epsilon$ with $\epsilon \sim N(0,\sigma_n^2)$, $f \sim GP(0,k(time,time^{'}))$

```{r}

# Subset data
temp <- dataQ2$temp[Stime]
# Find the noise value
polyFit <- lm(temp ~  Stime + Stime^2)
sigmaNoise = sd(polyFit$residuals)
MaternFunc2 = Matern32(sigmaf = 20, ell = 0.2)

# Fit the GP with built in Square expontial kernel (called rbfdot in kernlab)
GPfit <- gausspr(Stime, temp, kernel = MaternFunc2, var = sigmaNoise^2)
postMean2 <- predict(GPfit,Stime)
```

The plot below shows the observations plotted with a fitted posterior mean at each data point. The model fits the data quite well as it predicts the underlying trends (periodicity).

```{r}
plot(Stime, temp, ylim=c(-30,40),xlab="time",ylab="temperature")
lines(Stime,postMean2, col="red", lwd = 2)
legend("topright",c("Observations","Post. Mean"),
        col = c("black","red"), pch = c(1,-1), lty = c(0, 1), lwd=c(0,2))

```

### Part 3

```{r}
x<-scale(Stime)
xs<-scale(Stime) # XStar
n <- length(x)
SEkernel <- MaternFunc2
Kss <- kernelMatrix(kernel = SEkernel, x = xs, y = xs)
Kxx <- kernelMatrix(kernel = SEkernel, x = x, y = x)
Kxs <- kernelMatrix(kernel = SEkernel, x = x, y = xs)
Covf = Kss-t(Kxs)%*%solve(Kxx + sigmaNoise^2*diag(n), Kxs) # Covariance matrix of fStar
```

From the plot below, we see that the probability bands (the uncertainty of the function f, which is the MEAN of y) and the prediction bands (the uncertainty of individual y values. Here we are uncertain for two reasons: we don't know f at the test point, and we don't know the error (epsilon) that will hit this individual observation) fit the data quite well. Though, the prediction bands are quite wide as it contains more uncertainty.

```{r}

plot(Stime, temp, ylim=c(-30,65),xlab="time",ylab="temperature")
lines(Stime,postMean2, col="red", lwd = 2)
# Probability intervals for fStar
lines(Stime, postMean2 - 1.96*sqrt(diag(Covf)), col = "green")
lines(Stime, postMean2 + 1.96*sqrt(diag(Covf)), col = "green")
# Prediction intervals for yStar
lines(Stime, postMean2 - 1.96*sqrt((diag(Covf) + sigmaNoise^2)), col = "purple")
lines(Stime, postMean2 + 1.96*sqrt((diag(Covf) + sigmaNoise^2)), col = "purple")
legend("topright",c("Observations","Post. Mean","Prob. bands","Pred. bands"),
        col = c("black","red","green","purple"), pch = c(1,-1,-1,-1), lty = c(0, 1,1,1), lwd=c(0,2,2,2))


```


### Part 4

The following model is used for obtaining the posterior mean:

$temp = f(day) + \epsilon$ with $\epsilon \sim N(0,\sigma_n^2)$, $f \sim GP(0,k(day,day^{'}))$


```{r}
# Find the noise value
polyFit4 <- lm(temp ~  rep.int(Sday,6) )
sigmaNoise4 = sd(polyFit4$residuals)

# Fit the GP with built in Square expontial kernel (called rbfdot in kernlab)
GPfit4 <- gausspr(rep.int(Sday,6), temp, kernel = MaternFunc2, var = sigmaNoise4^2)
postMean4 <- predict(GPfit4,rep.int(Sday,6))
```

From the plot, we see that both models performed quite well and the posterior means follow closely the seasonality of the temperatures. The _Time_ model was fitting to the all observed information, hence it shows the overall trend of the temperature fluctuations over the whole time period. The _Day_ model captures the seasonality during the year, as the variable $1,2,...,365$ was repeated 6 times and fitted with the temperatures. It fits exactly the same curve 6 times.


```{r}

plot(Stime, temp,xlab="time",ylab="temperature",ylim=c(-30,45))
lines(Stime,postMean2, col="red", lwd = 2)
lines(Stime,postMean4, col="blue", lwd = 2)
legend("topright",c("Observations","Post. Mean(time)","Post. Mean(day)"),
        col = c("black","red", "blue"), pch = c(1,-1,-1), lty = c(0, 1,1), lwd=c(0,2,2))

```

### Part 5

The GP with a generalization of the periodic kernel is used in this part to model the temperature against the time. The periodic kernel is given by:
$$k(x,x')=\sigma_f^2\exp{\{-\frac{2\sin^2(pi|x-x'|)/d}{\ell_1^2}\}}\exp{\{-\frac{|x-x'|^2}{2\ell_2^2}\}}$$

```{r}
# Function that returns kernel class object
Matern5 <- function(sigmaf, ell1, ell2, d){
  val <- function(x1, x2){
    n1 <- length(x1)
    n2 <- length(x2)
    K <- matrix(NA,n1,n2)
    for (i in 1:n2){
      K[,i] <- sigmaf^2*exp(-2*sin(pi*abs(x1-x2[i])/d)^2/ell1^2)*exp(-0.5*abs(x1-x2[i])^2/ell2^2)
    }
    return(K)
  }
  class(val) <- "kernel"
  return(val)  
}

MaternFunc5 <- Matern5(20, 1, 10, 365/sd(time))
GPfit5 <- gausspr(Stime, temp, kernel = MaternFunc5, var = sigmaNoise^2)
postMean5 <- predict(GPfit5,Stime)
```

From the plot, we see that the posterior mean from _Time_ model predicts the underlying trends over all time period, the _Day_ model is good at capturing the yearly seasonality. The _Periodic_ model of this part seems to be performing quite well too. It seems the model with periodic kernel captures the overall trend as the model with the squared kernel with variable _Time_. But also it shows the average year seasonality as the model with squared kernel and variable _Day_. Since periodic kernel has one more length scale which controls the correlation between the same day in different years, this result looks reasonable. 

```{r}
plot(Stime, temp,ylim=c(-30,45),xlab="time",ylab="temperature")
lines(Stime,postMean2, col="red", lwd = 2)
lines(Stime,postMean4, col="blue", lwd = 2) 
lines(Stime,postMean5, col="green", lwd = 2) 
legend("topright",c("Observations","Post. Mean(time)","Post. Mean(day)","Post. Mean(periodic)"),
        col = c("black","red","blue","green"), pch = c(1,-1,-1,-1), lty = c(0, 1,1,1), lwd=c(0,2,2,2))
```


## Question 3

```{r}
# read data from github
data <- read.csv("https://github.com/STIMALiU/AdvMLCourse/raw/master/GaussianProcess/Code/banknoteFraud.csv",
                 header=FALSE, sep=",")
names(data) <- c("varWave","skewWave","kurtWave","entropyWave","fraud")
data[,5] <- as.factor(data[,5])
head(data)
set.seed(111); 
SelectTraining <- sample(1:dim(data)[1], size = 1000,
                                        replace = FALSE)
training <- data[SelectTraining,]
testing <- data[-SelectTraining,]

accuracy <- function (conf_mat){(conf_mat[1,1] + conf_mat[2,2]) / sum(conf_mat)}
```

### Part 1

The package _kenrlab_ is used to fit the Gaussian Process Classification on Fraud data-set.


```{r,message=FALSE,warning=FALSE}
GPfitFraud <- gausspr(fraud ~ varWave + skewWave, data=training)

# class probabilities 
probPreds <- predict(GPfitFraud, training, type="probabilities")
x1 <- seq(min(training[,1]),max(training[,1]),length=100)
x2 <- seq(min(training[,2]),max(training[,2]),length=100)
gridPoints <- meshgrid(x1, x2)
gridPoints <- cbind(c(gridPoints$x), c(gridPoints$y))

gridPoints <- data.frame(gridPoints)
names(gridPoints) <- names(training)[1:2]
probPreds <- predict(GPfitFraud, gridPoints, type="probabilities")

```

The model gives the corresponding confusion matrix on the training data: 
```{r}
# predict on the testing set
pred1 <- predict(GPfitFraud,training)
conM1 <- table("Prediction"=pred1, "True"=training$fraud) # confusion matrix
conM1

```

Hence, the accuracy is `r accuracy(conM1)`. From the plot below, we see that classifier works quite well and there are two contour peaks that concentrate the most of the points from the different class. However, the overlap region is quite big, and that explains the lower accuracy rate of the classifier.

```{r}
# Plotting for Prob(setosa)
contour(x1,x2,matrix(probPreds[,1],100,byrow = TRUE), 20, xlab = "varWave",
        ylab = "skewWave", main = 'Contour of Prediction Probabilities')
points(training[training[,5]==0,1],training[training[,5]==0,2],col="blue")
points(training[training[,5]==1,1],training[training[,5]==1,2],col="red")
legend("bottomright", legend=c("0","1"), pch=c(20,20), col=c("red","blue"))
```


### Part 2

The confusion matrix on the testing data is:
```{r}
# predict on the testing set
predT <- predict(GPfitFraud,testing)
conM2 <- table("Prediction"=predT, "True"=testing$fraud) # confusion matrix
conM2


```


The accuracy is `r accuracy(conM2)`, which is pretty close to the accuracy rate obtained on the training data. This is a good indication that the classification works well, and it is not overfitted.

### Part 3

In this part all available information in the data-set is used to classify the fraud. The resulting confusion matrix on the test data is:

```{r}
GPfitFraud3 <- gausspr(fraud ~ ., data=training)

# predict on the testing set
pred3 <- predict(GPfitFraud3,testing)
conM3 <- table("Prediction"=pred3, "True"= testing$fraud) # confusion matrix
conM3

```

The accuracy is `r accuracy(conM3)`, which is higher than in the previous parts, and very close to 1. It is reasonable that the classification model improved once we included all the available information. 

#Appendix

```{r, ref.label=knitr::all_labels(),echo=TRUE,eval=FALSE}

```


